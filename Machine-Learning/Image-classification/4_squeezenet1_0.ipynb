{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myseed = 6666  # set a random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, tfm, path=\"food-11\", isTrain=True):\n",
    "        super(FoodDataset).__init__()\n",
    "        if isTrain:\n",
    "            self.train_path = path + \"/train\"\n",
    "            self.valid_path = path + \"/valid\"\n",
    "            self.files = [self.train_path + \"/\" + x for x in os.listdir(self.train_path) if x.endswith(\".jpg\")]\n",
    "            self.files += [self.valid_path + \"/\" + x for x in os.listdir(self.valid_path) if x.endswith(\".jpg\")]\n",
    "            np.random.shuffle(self.files)\n",
    "        else:\n",
    "            self.path = path + \"/test\"\n",
    "            self.files = sorted([self.path + \"/\" + x for x in os.listdir(self.path) if x.endswith(\".jpg\")])\n",
    "\n",
    "        self.transform = tfm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        im = Image.open(fname)\n",
    "        im = self.transform(im)\n",
    "\n",
    "        try:\n",
    "            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n",
    "        except:\n",
    "            label = -1  # test has no label\n",
    "\n",
    "        return im, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "\n",
    "class SqueezeNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        self.squeezenet = models.squeezenet1_0(weights=None, progress=True)\n",
    "        self.squeezenet.classifier[1] = nn.Conv2d(512, n_class, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeezenet(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64\n",
    "n_epochs = 100\n",
    "\n",
    "patience = 8  # If no improvement in 'patience' epochs, early stop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FoodDataset(train_tfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_exp_name = \"squeezenet1_0\"\n",
    "fold_idx = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer):\n",
    "    isPrint = False\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if isPrint == False:\n",
    "            lr = param_group[\"lr\"]\n",
    "            print(f\"--- Learning rate decreases from {lr:.6f} to {lr * 0.8:.6f}. ---\")\n",
    "            isPrint = True\n",
    "        param_group[\"lr\"] = param_group[\"lr\"] * 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(dataset)):\n",
    "    if fold != fold_idx:\n",
    "        continue\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0, pin_memory=True )\n",
    "    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=0, pin_memory=True )\n",
    "\n",
    "    model = SqueezeNet(11).to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    \n",
    "    # Initialize trackers, these are not parameters and should not be changed\n",
    "    stale = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if stale > 5:\n",
    "            adjust_learning_rate(optimizer)\n",
    "\n",
    "        # ---------- Training ----------\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_accs = []\n",
    "\n",
    "        with tqdm(total=len(train_loader), unit=\"batch\") as tqdm_bar:\n",
    "            tqdm_bar.set_description(f\"Epoch {epoch + 1:03d}/{n_epochs:03d}\")\n",
    "            for batch in train_loader:\n",
    "                imgs, labels = batch\n",
    "\n",
    "                # Forward the data.\n",
    "                logits = model(imgs.to(device))\n",
    "\n",
    "                # Calculate the cross-entropy loss.\n",
    "                loss = criterion(logits, labels.to(device))\n",
    "\n",
    "                # Gradients stored in the parameters in the previous step should be cleared out first.\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute the gradients for parameters.\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip the gradient norms for stable training.\n",
    "                grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "                # Update the parameters with computed gradients.\n",
    "                optimizer.step()\n",
    "\n",
    "                # Compute the accuracy for current batch.\n",
    "                acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "                # Record the loss and accuracy.\n",
    "                train_loss.append(loss.item())\n",
    "                train_accs.append(acc)\n",
    "\n",
    "                tqdm_bar.update(1)\n",
    "                tqdm_bar.set_postfix(loss=f\"{sum(train_loss)/len(train_loss):.5f}\", acc=f\"{sum(train_accs) / len(train_accs):.5f}\", val_loss=f\"{0:.5f}\", val_acc=f\"{0:.5f}\")\n",
    "\n",
    "            train_loss = sum(train_loss) / len(train_loss)\n",
    "            train_acc = sum(train_accs) / len(train_accs)\n",
    "            tqdm_bar.set_postfix(loss=f\"{train_loss:.5f}\", acc=f\"{train_acc:.5f}\", val_loss=f\"{0:.5f}\", val_acc=f\"{0:.5f}\")\n",
    "\n",
    "            # ---------- Validation ----------\n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            valid_accs = []\n",
    "\n",
    "            for batch in valid_loader:\n",
    "                imgs, labels = batch\n",
    "\n",
    "                # Using torch.no_grad() accelerates the forward process.\n",
    "                with torch.no_grad():\n",
    "                    logits = model(imgs.to(device))\n",
    "\n",
    "                # We can still compute the loss (but not the gradient).\n",
    "                loss = criterion(logits, labels.to(device))\n",
    "\n",
    "                # Compute the accuracy for current batch.\n",
    "                acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "                # Record the loss and accuracy.\n",
    "                valid_loss.append(loss.item())\n",
    "                valid_accs.append(acc)\n",
    "\n",
    "                tqdm_bar.set_postfix(\n",
    "                    loss=f\"{train_loss:.5f}\", acc=f\"{train_acc:.5f}\", val_loss=f\"{sum(valid_loss) / len(valid_loss):.5f}\", val_acc=f\"{sum(valid_accs) / len(valid_accs):.5f}\"\n",
    "                )\n",
    "\n",
    "            # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "            valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "            valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "\n",
    "            tqdm_bar.set_postfix(loss=f\"{train_loss:.5f}\", acc=f\"{train_acc:.5f}\", val_loss=f\"{valid_loss:.5f}\", val_acc=f\"{valid_acc:.5f}\")\n",
    "            tqdm_bar.close()\n",
    "\n",
    "        # update logs\n",
    "        if valid_acc > best_acc:\n",
    "            with open(f\"./{_exp_name}_log.txt\", \"a\") as f:\n",
    "                f.write(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\\n\")\n",
    "        else:\n",
    "            with open(f\"./{_exp_name}_log.txt\", \"a\") as f:\n",
    "                f.write(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\\n\")\n",
    "\n",
    "        # save models\n",
    "        if valid_acc > best_acc:\n",
    "            print(f\"Best model found at epoch {epoch+1}, saving model\")\n",
    "            torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\")  # only save best to prevent output memory exceed error\n",
    "            best_acc = valid_acc\n",
    "            stale = 0\n",
    "        else:\n",
    "            stale += 1\n",
    "            if stale > patience:\n",
    "                print(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct test datasets.\n",
    "test_set = FoodDataset(test_tfm, isTrain=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Time Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "tta_num = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and generate prediction CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = SqueezeNet(11).to(device)\n",
    "model_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\"))\n",
    "model_best.eval()\n",
    "\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for data, _ in tqdm(test_loader):\n",
    "        for img in data:\n",
    "            test_input = img.view(1, 3, 224, 224)\n",
    "            test_pred = model_best(test_input.to(device))\n",
    "            test_pred = test_pred.cpu().data.numpy()\n",
    "\n",
    "            # test time augmentation\n",
    "            tta_pred = np.zeros((1, 11))\n",
    "            for _ in range(tta_num):\n",
    "                test_augmented = tta_transform(img)\n",
    "                test_augmented = test_augmented.view(1, 3, 224, 224)\n",
    "                pred = model_best(test_augmented.to(device))\n",
    "                tta_pred = tta_pred + pred.cpu().data.numpy()\n",
    "            tta_pred = tta_pred / tta_num\n",
    "            \n",
    "            # final prediction\n",
    "            test_label = np.argmax(test_pred * 0.7 + tta_pred * 0.3)\n",
    "\n",
    "            prediction.append(test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test csv\n",
    "def pad4(i):\n",
    "    return \"0\" * (4 - len(str(i))) + str(i)\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"Id\"] = [pad4(i) for i in range(len(test_set))]\n",
    "df[\"Category\"] = prediction\n",
    "df.to_csv(f\"{_exp_name}_submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
